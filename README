The Transformer model has great success in the field of speech recognition due to its powerful contextual modeling capability. However, 
Transformer decoder uses encoder output features with redundant information, which limits the further improvement of the model decoding 
speed and is not conducive to the wider application of the model. 
   Therefore, we proposes a Transformer decoding acceleration method by compressing acoustic feature sequences, which is called 
Redundant block discarding (DRB). It uses the spike sequence generated by the temporal concatenation classifier (CTC) to remove consecutive
redundant blank frames from the encoder output features to reduce the feature length required by the decoder, thus reducing the decoder computation
and achieving the goal of improving the decoding speed of the model.
   Our method is implemented in the WENET speech recognition toolbox. At present, only the private core code segment of DRB method is released, 
and more complete code structure will be released for your reference in the future.
